---
title: "How to Manufacture Your Personality to Gain Friends and Second Dates: A Data-Driven Field Guide for Pathetic People by Pathetic People"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Group members: Beau, Richard, Saad, Kevin

```{r}
library(data.table)
```


# Importing the data
(credit: Beau and Richard)

Speed Dating dataset: https://www.kaggle.com/annavictoria/speed-dating-experiment

Each row of the data represents one date between two people. The primary key is one of the participants in the date, so for each date there are two rows in the dataset.

We imported the data, and then turned the appropriate columns into factors. The missing data came as blank strings, which we encoded as NAs.

For now, we are filtering out rows which do not have complete data (some participants did not include their hometown or undergraduate school, which were used as proxies to quantify wealth and education level). We are probably going to add these partially-full rows back into our analysis as we drop features from our model. TODO check if true

```{r}
# This works if you do the following:
# stick the csv file in the same directory as this Rmd
# in RStudio, go to Session -> Set Work Directory -> To Source File location
# this expects only one csv to be in the directory
project.dir <- getwd()
setwd(project.dir)
csv_file <- list.files(file.path(project.dir), pattern="*.csv", full.names=TRUE) 
full_data <- fread(file.path(csv_file))


full_data[,c("race_o","field_cd", "race", "goal", "date", "go_out", "career_c", "met", "dec", "length", "numdat_2", "gender", "condtn")] <- lapply(full_data[,c("race_o","field_cd", "race", "goal", "date", "go_out", "career_c", "met", "dec", "length", "numdat_2", "gender", "condtn")], factor)

#income, tuition, and mn_sat have NA values so they aren't numeric

nrow(full_data[complete.cases(full_data[ , c("income","tuition","mn_sat")]),])
nrow(full_data)

#we could look at subset? is there a reason that people left off this information?

#full_data <- nrow(full_data[complete.cases(full_data[ , c("income")]),])
#nrow(full_data[complete.cases(full_data[ , c("mn_sat", "income")]),])
#nrow(full_data[complete.cases(full_data[ , c("tuition", "income")]),])

```

Save/Load current state for easy continuation
(credit: Kev)
```{r}
#setwd()
#saveRDS(full_data, file="full_speed_dating.rds")
#full_data <- readRDS("full_speed_dating.rds")
```


# Predicting rate of people liking you with constant fields
(credit: Beau and Richard)


The variable 'dec_o' encodes whether the other person in the date decided they would like a second date with you (1 for yes, 0 for no). We decided to use this as the response variable instead of 'match' because 'match' means that both people mutually liked each other. In particular, for each participant, we took the mean of 'dec_o' as a proxy for desireable-ness.


We defined a new table 'const_attribs' which represents the constant attributes of the participants. For example, from one date to another your age, field of study, and gender do not change. We wanted to see whether we can predict desirable-ness from your non-subjective attributes.

However, we could not predict anything with this lm (R^2 = 0.14).

```{r}
#  rate that people decide they wanted to date you after the speed date
# aggregate certain variables for each individual (avg's of attracitness, intelligence score ... etc. that others gave after meeting)

desirableness <- aggregate(dec_o ~ iid, full_data, mean)


# create subset with the fields that remain constant for an individual (regardless of the meeting)
const_attribs <- full_data[, c("iid", "gender","age", "field_cd", "mn_sat", "tuition", "imprace", "income", "goal", "date", "go_out", "career_c", "match_es","sports", "tvsports", "exercise", "dining", "museums", "art", "hiking", "gaming", "clubbing", "reading", "tv", "theater", "movies", "concerts", "music", "shopping", "yoga")]

# this resulting table has lots of duplicated data
# remove duplicates so there is only one obs per person
const_attribs <- const_attribs[!duplicated(const_attribs),]

# binding back together w desirableness
const_attribs <- merge(desirableness, const_attribs, by="iid")

# merges list of dataframes into a single df. removes iid as a predictor.
full_const_fit <- lm(dec_o ~ .-iid, data = na.omit(const_attribs))  # TODO find reduced model in git commit history
```

After we reduced the insignificant predictors in this model, it had an adjusted R^2 of just 0.14. 


# Predicting rate of people liking you with constant fields and partner-rankings

We augmented this linear model using the rankings from your partners: how attractive, sincere, intelligent, fun, and amibiotous your partners percieved you to be durin the 4 minute speed-date. In particular, for each individual we took the mean rating by their partners.

```{r}
#additional explanitory variables. note that after this point, attr_o, sinc_o, etc are
# now aggregated means
attractive <- aggregate(attr_o ~ iid, full_data, mean)
sincere <- aggregate(sinc_o ~ iid, full_data, mean)
intelligent <- aggregate(intel_o ~ iid, full_data, mean)
fun <- aggregate(fun_o ~ iid, full_data, mean)
ambitious <- aggregate( amb_o ~ iid, full_data, mean) # this was assigned incorrectly.. do any of the analyses change in model prediction section?
attractive <- aggregate(attr_o ~ iid, full_data, mean)
likeability <- aggregate(like_o ~ iid,full_data, mean)

augmented_attribs <- Reduce(function(x, y) merge(x, y, all=TRUE), list(attractive, sincere, intelligent, fun, ambitious, likeability, const_attribs, desirableness))
```

We removed a few columns that had pesky numerical values. We will come back and probably add these back in after fixing that.
```{r}
# remove fields that consist of mostly NA's
# these are tuition, SAT, and predicted income
augmented_attribs <- augmented_attribs[, -which(names(augmented_attribs) %in% c("tuition", "mn_sat", "income")) ]
# fit a full model
# NOTE: we removed  NAs, remember to add these rows back later if we reduce the model
full_augmented_fit <- lm(dec_o ~ .-iid, data = na.omit(augmented_attribs))
summary(full_augmented_fit)
```
```{r}
plot(full_augmented_fit)
```

The full model has an adj-R^2 of 0.69 on 76 predictor variables. In order to reduce our model, we ran a stepwise AIC.

```{r}
library(MASS)
step <- stepAIC(full_augmented_fit, direction="both")
step$anova # display results
```

Reducing our desirableness model based on our stepAIC:
```{r}
# the stepAIC returned this model as these as our optimal predictors
# attr_o + sinc_o + intel_o + like_o + gender + career_c + tvsports + gaming + yoga
# dec_o ~ attr_o + fun_o + gender + age + field_cd + career_c + tvsports + museums + yoga
# dec_o, attr_o, fun_o here is the mean of dec_o, attr_o, fun_o reported by partners
reduced_augmented_fit <- lm(dec_o ~ attr_o + sinc_o + intel_o + like_o + gender + career_c + tvsports + gaming + yoga, data = na.omit(augmented_attribs))

summary(reduced_augmented_fit)
```
Our model stayed with approximately the same adjusted R^2 (0.70), but now we have only 24 predictors. We noticed that career_c and field_cd did not improve our R^2 by much, so we ran an F-test
```{r}
anova(reduced_augmented_fit, lm(dec_o ~ attr_o + fun_o + gender + age + 
    tvsports + museums + yoga, data = na.omit(augmented_attribs)) )
```
The F test shows it the model benefits from career_c and field_cd, so we will keep them it in model.

# Checking for associations between variables
```{r}
# dec_o ~ attr_o + fun_o + gender + age + field_cd + career_c + tvsports + museums + yoga
pairs(augmented_attribs[,c("dec_o", "sinc_o", "attr_o", "gender","age", "like_o", "career_c", "tvsports", "gaming","yoga")]) 
#pairs(augmented_attribs[,c("dec_o", "fun_o", "attr_o", "gender","age", "field_cd", "career_c", "tvsports","museums","yoga")]) 
# it appears that fun_o and attractiveness_o have a linear relationship with our response AND (maybe) each other

par(mfrow = c(2,2))
plot(reduced_augmented_fit)
# QQ plot indicates normally distributed errors. The scale-location plot show that there  and possibly-maybe some heteroskedasticity in our variance.


#next step is to maybe introduce interaction terms?
```

The residuals plot shows that the variance is non-heteroskedastic. The Q-Q plot tells us that we have normally distributed errors. 

# How can we get dates?
(credit: Kev)
Based on the model identified above, we want to help you get a date. 
What attributes predict fun or attractiveness?
Are we any good at knowing how fun/attractive we are?

# Predicting likeability
I deliberately removed attractiveness to see what new insights we can find (attractiveness positively correlates with everything). I also removed the desirability metric, which was also strongly linearly correlated with likeability.
```{r}
like_lm_full <- lm(like_o ~ .-iid -dec_o -attr_o, data = na.omit(augmented_attribs))
summary(like_lm_full)
```
```{r}
step_like <- stepAIC(like_lm_full, direction="both")
```

```{r}
step_like$anova # display results
```

Reducing our likeability prediction model based on our stepAIC:
```{r}
## like_o ~ sinc_o + intel_o + fun_o + amb_o + gender + age + sports + 
##     gaming + reading + tv + theater + movies
like_lm_reduced <- lm(like_o ~ sinc_o + intel_o + fun_o + amb_o + gender + age + sports + gaming + reading + tv + theater + movies, data = na.omit(augmented_attribs))

summary(like_lm_reduced)
```
This likeability model has an adj-R^2 of 0.7693 with just 12 predictors.

```{r}
pairs(augmented_attribs[,c("like_o", "sinc_o", "intel_o", "fun_o", "amb_o",  "age", "reading", "theater", "movies", "gender", "sports",  "gaming", "tv")])
# removed gender, since it is a factor
# removed "theater", "movies", "sports",  "gaming", "tv" to declutter
library("corrplot")
corrplot(na.omit(augmented_attribs[,c("like_o", "sinc_o", "intel_o", "fun_o", "amb_o",  "age", "reading")]), method = "color")
```


# So what makes us fun? 

There is strong colinearity among all of the perceived attributes (the more fun you are the more attrictive you are). We will exclude the predictors for now. It is possible that the more attractive you are, the more fun/sincere you are perceived or vice versa.

```{r}
fun_augmented_fit <- lm(fun_o ~ .-iid -attr_o -sinc_o -intel_o -dec_o -amb_o, data = na.omit(augmented_attribs))
summary(fun_augmented_fit)
```

```{r}
# reduce model with stepAIC
step <- stepAIC(fun_augmented_fit, direction="both")
step$anova # display results
```

Reducing our model based on our anova results
```{r}
# fun_o ~ gender + go_out + career_c + sports + exercise + dining + gaming + clubbing + tv + concerts + music
fun_reduced_augmented_fit <- lm(fun_o ~ gender + go_out + career_c + sports + exercise + dining + 
    gaming + clubbing + tv + concerts + music, data = na.omit(augmented_attribs))
summary(fun_reduced_augmented_fit)
```
The R^2 is just 0.1557.

```{r}
# fun_o ~ gender + go_out + career_c + sports + exercise + dining + gaming + clubbing + tv + concerts + music
pairs(augmented_attribs[,c("fun_o", "gender", "go_out", "career_c", "sports", "exercise", "dining", 
    "gaming", "clubbing", "tv", "concerts", "music")])

#that didn't do shite
par(mfrow = c(2,2))
plot(fun_reduced_augmented_fit)

```

Does the model come out any better if we go forwards?
```{r}
# I don't want the forward approach to grab the perceived data
smaller_augmented_attribs <- augmented_attribs[, -which(names(augmented_attribs) %in% c("attr_o", "sinc_o", "intel_o", "dec_o")) ]
# since we have a bunch of predictor variables I'm going to run a stepwaise AIC
fun_augmented_fit_2 <- lm(fun_o ~ 1, data = na.omit(smaller_augmented_attribs))
summary(fun_augmented_fit_2)
step <- stepAIC(fun_augmented_fit_2, direction="forward", scope = list(upper = fun_augmented_fit, lower = fun_augmented_fit_2))
step$anova # display results
```

Reducing our model based on our anova results
```{r}
# Forward AIC = fun_o ~ go_out + exercise + clubbing + gender + field_cd + tv
fun_reduced_augmented_fit_2 <- lm(fun_o ~ go_out + exercise + clubbing + gender + field_cd + tv, data = na.omit(smaller_augmented_attribs))
summary(fun_reduced_augmented_fit_2)
```
The adjusted R^2 is quite low, at 0.1403 with 27 variables.
```{r}
pairs(augmented_attribs[,c("fun_o", "gender", "go_out", "exercise", "clubbing", "tv", "field_cd")])

par(mfrow = c(2,2))
plot(fun_reduced_augmented_fit_2)
```

Although these appear statistically significant, the Adj. R. Square for this model is very low and there is not much linear relationship from the predicted values
```{r}
plot(augmented_attribs$fun_o ~ augmented_attribs$exercise)
plot(augmented_attribs$fun_o ~ augmented_attribs$tv)
plot(augmented_attribs$fun_o ~ augmented_attribs$gender)
plot(augmented_attribs$fun_o ~ augmented_attribs$go_out)
plot(augmented_attribs$fun_o ~ augmented_attribs$clubbing)
plot(augmented_attribs$fun_o ~ augmented_attribs$field_cd)
plot(augmented_attribs$fun_o ~ augmented_attribs$career_c)
```

Where should we go from here with this idea?

- perceived fun for males is larger range than for females?
- real estate agents are more fun?
- TO-DO: show plot for correlation between fun and attractiveness
- visually, it is possible that the less you go out (7) the less fun you are
- it could be possible that the more important clubbing is to you, the more fun you are
- t-tests to show which careers, fields of study are more attractive/fun/etc?
- TO-DO: we need to try splitting all of our analyses by genders (what men what/think is different than women... the data is already very binary so this is based on this dataset)
- TO-DO: make plots prettier
- TO-DO: do similar thing with attractiveness
- TO-DO: hypothesis testing

# How accurate is our self-perception?

We see that personality traits like fun and attractiveness are correlated with if your speed date decides to say "yes" to you. What does this mean for us as far as what we think about ourselves? Do we even know how fun/attractive we are to others?

Alternatively, are others able to pick up on our less obvious traits, like sincerity and ambition, in 4 minutes?

plot self ratings vs. avg of trait
```{r}
#  rate that people decide they like you after the speed date
desirableness <- aggregate(dec_o ~ iid, full_data, mean)

#additional explanitory variables
attractive <- aggregate(attr_o ~ iid, full_data, mean)
sincere <- aggregate(sinc_o ~ iid, full_data, mean)
intelligent <- aggregate( intel_o ~ iid, full_data, mean)
fun <- aggregate( fun_o ~ iid, full_data, mean)
ambitious <- aggregate( amb_o ~ iid, full_data, mean)

personality_attribs <- full_data[, c("iid", "attr3_1", "sinc3_1", "fun3_1", "intel3_1", "amb3_1")]

personality_attribs <- personality_attribs[!duplicated(personality_attribs),]

# binding back together
personality_attribs <- merge(desirableness, personality_attribs,by="iid")

# we end up with three columns of dec_o here
personality_attribs <- Reduce(function(x, y) merge(x, y, all=TRUE), list(attractive, sincere, intelligent, fun, ambitious, personality_attribs, desirableness  ))


attr_lm <- lm(personality_attribs$attr_o ~ personality_attribs$attr3_1)
summary(attr_lm)

plot(personality_attribs$attr_o ~ personality_attribs$attr3_1)
abline(attr_lm)

library(ggplot2)
personality_attribs$attr3_1 <- as.factor(personality_attribs$attr3_1)
p <- ggplot(na.omit(personality_attribs), aes(x=attr3_1, y=attr_o, fill=attr3_1)) + 
  geom_violin(trim=FALSE)
# plot with median and quartile
p  + geom_boxplot(width=0.1, fill="white") + labs(title="Perceived Attractiveness", x="Self-Reported", y="Partner Rating") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))
# is it possible to add the lm to this plot?

```


```{r}
sinc_lm <- lm(personality_attribs$sinc_o ~ personality_attribs$sinc3_1)
intel_lm <- lm(personality_attribs$intel_o ~ personality_attribs$intel3_1)
fun_lm <- lm(personality_attribs$fun_o ~ personality_attribs$fun3_1)
amb_lm <- lm(personality_attribs$amb_o ~ personality_attribs$amb3_1)

summary(sinc_lm)
summary(intel_lm)
summary(fun_lm)
summary(amb_lm)

par(mfrow = c(2,2))
plot(personality_attribs$sinc_o ~ personality_attribs$sinc3_1)
abline(sinc_lm)
plot(personality_attribs$fun_o ~ personality_attribs$fun3_1)
abline(fun_lm)
plot(personality_attribs$intel_o ~ personality_attribs$intel3_1)
abline(intel_lm)
plot(personality_attribs$amb_o ~ personality_attribs$amb3_1)
abline(amb_lm)
```

```{r}
personality_attribs$sinc3_1 <- as.factor(personality_attribs$sinc3_1)
personality_attribs$fun3_1 <- as.factor(personality_attribs$fun3_1)
personality_attribs$intel3_1 <- as.factor(personality_attribs$intel3_1)
personality_attribs$amb3_1 <- as.factor(personality_attribs$amb3_1)

ggplot(na.omit(personality_attribs), aes(x=sinc3_1, y=sinc_o, fill=sinc3_1)) + 
  geom_violin(trim=FALSE) + geom_boxplot(width=0.1, fill="white") + labs(title="Perceived Sincerity", x="Self-Reported", y="Partner Rating") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))
ggplot(na.omit(personality_attribs), aes(x=fun3_1, y=sinc_o, fill=fun3_1)) + 
  geom_violin(trim=FALSE) + geom_boxplot(width=0.1, fill="white") + labs(title="Perceived Fun", x="Self-Reported", y="Partner Rating") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))
ggplot(na.omit(personality_attribs), aes(x=intel3_1, y=sinc_o, fill=intel3_1)) + 
  geom_violin(trim=FALSE) + geom_boxplot(width=0.1, fill="white") + labs(title="Perceived Intelligence", x="Self-Reported", y="Partner Rating") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))
ggplot(na.omit(personality_attribs), aes(x=amb3_1, y=sinc_o, fill=amb3_1)) + 
  geom_violin(trim=FALSE) + geom_boxplot(width=0.1, fill="white") + labs(title="Perceived Ambition", x="Self-Reported", y="Partner Rating") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))
```

```{r}
library(tidyverse)
sincerity <- full_data[,c("sinc_o", "sinc3_1")]
sincerity <- sincerity %>% count(sinc3_1, sinc_o)
# round very few decimal ratings to integers
sincerity$sinc_o <- as.integer(sincerity$sinc_o)
sincerity$sinc3_1 <- as.factor(sincerity$sinc3_1)
sincerity$sinc_o <- as.factor(sincerity$sinc_o)

fun <- full_data[,c("fun_o", "fun3_1")]
fun <- fun %>% count(fun3_1, fun_o)
fun$fun3_1 <- as.factor(fun$fun3_1)
fun$fun_o <- as.factor(as.integer(fun$fun_o))

ambition <- full_data[,c("amb_o", "amb3_1")]
ambition <- ambition %>% count(amb3_1, amb_o)
ambition$amb3_1 <- as.factor(ambition$amb3_1)
ambition$amb_o <- as.factor(as.integer(ambition$amb_o))

intelligence <- full_data[,c("intel_o", "intel3_1")]
intelligence <- intelligence %>% count(intel3_1, intel_o)
intelligence$intel3_1 <- as.factor(intelligence$intel3_1)
intelligence$intel_o <- as.factor(as.integer(intelligence$intel_o))

hotness <- full_data[,c("attr_o", "attr3_1")]
hotness <- hotness %>% count(attr3_1, attr_o)
hotness$attr3_1 <- as.factor(hotness$attr3_1)
hotness$attr_o <- as.factor(as.integer(hotness$attr_o))

ggplot(data = na.omit(hotness), aes(x=attr3_1, y=n, fill=attr_o)) +
geom_bar(position=position_fill(reverse = TRUE), stat="identity") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))

ggplot(data = na.omit(intelligence), aes(x=intel3_1, y=n, fill=intel_o)) +
geom_bar(position=position_fill(reverse = TRUE), stat="identity") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))

ggplot(data = na.omit(ambition), aes(x=amb3_1, y=n, fill=amb_o)) +
geom_bar(position=position_fill(reverse = TRUE), stat="identity") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))

ggplot(data = na.omit(fun), aes(x=fun3_1, y=n, fill=fun_o)) +
geom_bar(position=position_fill(reverse = TRUE), stat="identity") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))

ggplot(data = na.omit(sincerity), aes(x=sinc3_1, y=n, fill=sinc_o)) +
geom_bar(position=position_fill(reverse = TRUE), stat="identity") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))
```

```{r}
# show with frequency
ggplot(data = na.omit(hotness), aes(x=attr3_1, y=n, fill=attr_o)) +
geom_bar(position=position_stack(reverse = TRUE), stat="identity") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))

ggplot(data = na.omit(intelligence), aes(x=intel3_1, y=n, fill=intel_o)) +
geom_bar(position=position_stack(reverse = TRUE), stat="identity") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))

ggplot(data = na.omit(ambition), aes(x=amb3_1, y=n, fill=amb_o)) +
geom_bar(position=position_stack(reverse = TRUE), stat="identity") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))

ggplot(data = na.omit(fun), aes(x=fun3_1, y=n, fill=fun_o)) +
geom_bar(position=position_stack(reverse = TRUE), stat="identity") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))

ggplot(data = na.omit(sincerity), aes(x=sinc3_1, y=n, fill=sinc_o)) +
geom_bar(position=position_stack(reverse = TRUE), stat="identity") + scale_fill_brewer(palette="Spectral", direction = -1) + guides(fill= guide_legend(reverse = TRUE))
```

Are people consistant across the board of ratings (is self-esteem correlated?)
```{r}
pairs(full_data[,c("fun3_1", "attr3_1", "sinc3_1", "amb3_1", "intel3_1")]) 
```
It looks like there may be a correlation between self-esteem in intelligence and attractiveness?
```{r}
attr_intel_lm <- lm(full_data$attr3_1 ~ full_data$intel3_1)
summary(attr_intel_lm)
plot(full_data$attr3_1 ~ full_data$intel3_1)
abline(attr_intel_lm)
```
No evidence of a correlation. 
How do I want to ask this.. if you gave yourself a 2 in some field, what was the average you gave yourself in the other fields?
    I would take the mean(self-ratings) and var(self-ratings) for each person and plot them against each other --Rich
    
Self-reported trait ratings do not correlate with average partner ratings.

General Takeaway: Turns out.. we kind of suck at predicting how attractive we are to others. What we think about ourselves is not at all correlated to what others see in us. Maybe there is no point to worrying about how strangers see us, because either they don't or we don't. Or maybe it's hard to guage how ambitious and sincere someone is in 4 minutes, but physical attractiveness and fun are easier to guage.
